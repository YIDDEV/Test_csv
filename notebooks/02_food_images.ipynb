{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e5fbce6",
   "metadata": {},
   "source": [
    "## Estimation des calories à partir des images alimentaires\n",
    "\n",
    "L'objectif est d'estimer, pour chaque participant et chaque jour,\n",
    "le nombre total de calories consommées à partir des images des repas.\n",
    "\n",
    "Compte tenu de la variabilité des images (angles, portions, qualité),\n",
    "l'approche retenue vise à capturer une **tendance journalière**\n",
    "plutôt qu'une estimation nutritionnelle exacte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66efa10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e29cc4e",
   "metadata": {},
   "source": [
    "### Limites de l'approche\n",
    "\n",
    "- Absence d'information sur les portions\n",
    "- Possibilité de plusieurs aliments dans une image\n",
    "- Erreurs de classification possibles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28309a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_image_datetime(img_path: Path):\n",
    "    \"\"\"\n",
    "    Extrait la date/heure depuis les métadonnées EXIF d'une image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        exif_data = img._getexif()\n",
    "\n",
    "        if exif_data is None:\n",
    "            return None\n",
    "\n",
    "        for tag_id, value in exif_data.items():\n",
    "            tag = TAGS.get(tag_id, tag_id)\n",
    "            if tag == \"DateTimeOriginal\":\n",
    "                return pd.to_datetime(value, format=\"%Y:%m:%d %H:%M:%S\")\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_food_images_metadata(participant_id: str, images_root: Path, project_root: Path):\n",
    "    \"\"\"\n",
    "    Charge les métadonnées des images alimentaires d'un participant\n",
    "    avec un chemin relatif au projet\n",
    "    \"\"\"\n",
    "    food_images_path = images_root / participant_id / \"food-images\"\n",
    "\n",
    "    if not food_images_path.exists():\n",
    "        print(f\"[INFO] No food-images folder for {participant_id}\")\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"participant_id\", \"image_path\", \"datetime\", \"date\"]\n",
    "        )\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for img_path in food_images_path.glob(\"*.jp*g\"):\n",
    "        dt = extract_image_datetime(img_path)\n",
    "\n",
    "        if dt is not None:\n",
    "            records.append({\n",
    "                \"participant_id\": participant_id,\n",
    "                \"image_path\": img_path.relative_to(project_root).as_posix(),\n",
    "                \"datetime\": dt,\n",
    "                \"date\": dt.date()\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2099069a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>datetime</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p01</td>\n",
       "      <td>data/raw/p01/food-images/IMG_8916.jpeg</td>\n",
       "      <td>2020-02-01 10:03:41</td>\n",
       "      <td>2020-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p01</td>\n",
       "      <td>data/raw/p01/food-images/IMG_8917.jpeg</td>\n",
       "      <td>2020-02-01 11:39:57</td>\n",
       "      <td>2020-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p01</td>\n",
       "      <td>data/raw/p01/food-images/IMG_8918.jpeg</td>\n",
       "      <td>2020-02-01 17:03:39</td>\n",
       "      <td>2020-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p01</td>\n",
       "      <td>data/raw/p01/food-images/IMG_8920.jpeg</td>\n",
       "      <td>2020-02-01 19:16:48</td>\n",
       "      <td>2020-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p01</td>\n",
       "      <td>data/raw/p01/food-images/IMG_8921.jpeg</td>\n",
       "      <td>2020-02-01 21:04:57</td>\n",
       "      <td>2020-02-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id                              image_path            datetime  \\\n",
       "0            p01  data/raw/p01/food-images/IMG_8916.jpeg 2020-02-01 10:03:41   \n",
       "1            p01  data/raw/p01/food-images/IMG_8917.jpeg 2020-02-01 11:39:57   \n",
       "2            p01  data/raw/p01/food-images/IMG_8918.jpeg 2020-02-01 17:03:39   \n",
       "3            p01  data/raw/p01/food-images/IMG_8920.jpeg 2020-02-01 19:16:48   \n",
       "4            p01  data/raw/p01/food-images/IMG_8921.jpeg 2020-02-01 21:04:57   \n",
       "\n",
       "         date  \n",
       "0  2020-02-01  \n",
       "1  2020-02-01  \n",
       "2  2020-02-01  \n",
       "3  2020-02-01  \n",
       "4  2020-02-01  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "participant_id = \"p01\"\n",
    "\n",
    "food_df = load_food_images_metadata(\n",
    "    participant_id=participant_id,\n",
    "    images_root=DATA_RAW,\n",
    "    project_root=PROJECT_ROOT\n",
    ")\n",
    "\n",
    "food_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b2c588",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37221d0c",
   "metadata": {},
   "source": [
    "# Model efficientnet b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e7d3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koula\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (conv_stem): Conv2dSame(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "  (bn1): BatchNormAct2d(\n",
       "    32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (drop): Identity()\n",
       "    (act): SiLU(inplace=True)\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): DepthwiseSeparableConv(\n",
       "        (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2dSame(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2dSame(144, 144, kernel_size=(5, 5), stride=(2, 2), groups=144, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2dSame(240, 240, kernel_size=(3, 3), stride=(2, 2), groups=240, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2dSame(672, 672, kernel_size=(5, 5), stride=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (gate): Sigmoid()\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn2): BatchNormAct2d(\n",
       "    1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (drop): Identity()\n",
       "    (act): SiLU(inplace=True)\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (classifier): Linear(in_features=1280, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "model = timm.create_model(\n",
    "    \"tf_efficientnet_b0\",\n",
    "    pretrained=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a5136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOOD101_CLASSES = [\n",
    "    \"apple_pie\",\"baby_back_ribs\",\"baklava\",\"beef_carpaccio\",\"beef_tartare\",\n",
    "    \"beet_salad\",\"beignets\",\"bibimbap\",\"bread_pudding\",\"breakfast_burrito\",\n",
    "    \"bruschetta\",\"caesar_salad\",\"cannoli\",\"caprese_salad\",\"carrot_cake\",\n",
    "    \"ceviche\",\"cheesecake\",\"cheese_plate\",\"chicken_curry\",\"chicken_quesadilla\",\n",
    "    \"chicken_wings\",\"chocolate_cake\",\"chocolate_mousse\",\"churros\",\n",
    "    \"clam_chowder\",\"club_sandwich\",\"crab_cakes\",\"creme_brulee\",\"croque_madame\",\n",
    "    \"cup_cakes\",\"deviled_eggs\",\"donuts\",\"dumplings\",\"edamame\",\"eggs_benedict\",\n",
    "    \"escargots\",\"falafel\",\"filet_mignon\",\"fish_and_chips\",\"foie_gras\",\n",
    "    \"french_fries\",\"french_onion_soup\",\"french_toast\",\"fried_calamari\",\n",
    "    \"fried_rice\",\"frozen_yogurt\",\"garlic_bread\",\"gnocchi\",\"greek_salad\",\n",
    "    \"grilled_cheese_sandwich\",\"grilled_salmon\",\"guacamole\",\"gyoza\",\n",
    "    \"hamburger\",\"hot_and_sour_soup\",\"hot_dog\",\"huevos_rancheros\",\"hummus\",\n",
    "    \"ice_cream\",\"lasagna\",\"lobster_bisque\",\"lobster_roll_sandwich\",\n",
    "    \"macaroni_and_cheese\",\"macarons\",\"miso_soup\",\"mussels\",\"nachos\",\n",
    "    \"omelette\",\"onion_rings\",\"oysters\",\"pad_thai\",\"paella\",\"pancakes\",\n",
    "    \"panna_cotta\",\"peking_duck\",\"pho\",\"pizza\",\"pork_chop\",\"poutine\",\n",
    "    \"prime_rib\",\"pulled_pork_sandwich\",\"ramen\",\"ravioli\",\n",
    "    \"red_velvet_cake\",\"risotto\",\"samosa\",\"sashimi\",\"scallops\",\n",
    "    \"seaweed_salad\",\"shrimp_and_grits\",\"spaghetti_bolognese\",\n",
    "    \"spaghetti_carbonara\",\"spring_rolls\",\"steak\",\"strawberry_shortcake\",\n",
    "    \"sushi\",\"tacos\",\"takoyaki\",\"tiramisu\",\"tuna_tartare\",\"waffles\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "805ddc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5edd423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_food(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    x = transform(img).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        top_prob, top_idx = probs.max(dim=1)\n",
    "\n",
    "    idx = top_idx.item()\n",
    "\n",
    "    if idx >= len(FOOD101_CLASSES):\n",
    "        return \"unknown\", float(top_prob)\n",
    "\n",
    "    return FOOD101_CLASSES[idx], float(top_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6643af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calories\n",
    "FOOD_CALORIES = {\n",
    "    \"apple_pie\": 320,\n",
    "    \"baby_back_ribs\": 650,\n",
    "    \"baklava\": 330,\n",
    "    \"beef_carpaccio\": 250,\n",
    "    \"beef_tartare\": 300,\n",
    "    \"beet_salad\": 180,\n",
    "    \"beignets\": 350,\n",
    "    \"bibimbap\": 550,\n",
    "    \"bread_pudding\": 380,\n",
    "    \"breakfast_burrito\": 550,\n",
    "    \"bruschetta\": 180,\n",
    "    \"caesar_salad\": 470,\n",
    "    \"cannoli\": 300,\n",
    "    \"caprese_salad\": 250,\n",
    "    \"carrot_cake\": 420,\n",
    "    \"ceviche\": 200,\n",
    "    \"cheesecake\": 430,\n",
    "    \"cheese_plate\": 450,\n",
    "    \"chicken_curry\": 500,\n",
    "    \"chicken_quesadilla\": 480,\n",
    "    \"chicken_wings\": 600,\n",
    "    \"chocolate_cake\": 450,\n",
    "    \"chocolate_mousse\": 350,\n",
    "    \"churros\": 400,\n",
    "    \"clam_chowder\": 250,\n",
    "    \"club_sandwich\": 550,\n",
    "    \"crab_cakes\": 350,\n",
    "    \"creme_brulee\": 330,\n",
    "    \"croque_madame\": 550,\n",
    "    \"cup_cakes\": 300,\n",
    "    \"deviled_eggs\": 200,\n",
    "    \"donuts\": 320,\n",
    "    \"dumplings\": 400,\n",
    "    \"edamame\": 190,\n",
    "    \"eggs_benedict\": 550,\n",
    "    \"escargots\": 220,\n",
    "    \"falafel\": 400,\n",
    "    \"filet_mignon\": 350,\n",
    "    \"fish_and_chips\": 700,\n",
    "    \"foie_gras\": 450,\n",
    "    \"french_fries\": 420,\n",
    "    \"french_onion_soup\": 300,\n",
    "    \"french_toast\": 400,\n",
    "    \"fried_calamari\": 450,\n",
    "    \"fried_rice\": 520,\n",
    "    \"frozen_yogurt\": 200,\n",
    "    \"garlic_bread\": 350,\n",
    "    \"gnocchi\": 450,\n",
    "    \"greek_salad\": 230,\n",
    "    \"grilled_cheese_sandwich\": 430,\n",
    "    \"grilled_salmon\": 420,\n",
    "    \"guacamole\": 230,\n",
    "    \"gyoza\": 380,\n",
    "    \"hamburger\": 550,\n",
    "    \"hot_and_sour_soup\": 250,\n",
    "    \"hot_dog\": 380,\n",
    "    \"huevos_rancheros\": 500,\n",
    "    \"hummus\": 250,\n",
    "    \"ice_cream\": 270,\n",
    "    \"lasagna\": 600,\n",
    "    \"lobster_bisque\": 380,\n",
    "    \"lobster_roll_sandwich\": 450,\n",
    "    \"macaroni_and_cheese\": 600,\n",
    "    \"macarons\": 300,\n",
    "    \"miso_soup\": 90,\n",
    "    \"mussels\": 300,\n",
    "    \"nachos\": 600,\n",
    "    \"omelette\": 350,\n",
    "    \"onion_rings\": 400,\n",
    "    \"oysters\": 150,\n",
    "    \"pad_thai\": 650,\n",
    "    \"paella\": 600,\n",
    "    \"pancakes\": 450,\n",
    "    \"panna_cotta\": 300,\n",
    "    \"peking_duck\": 700,\n",
    "    \"pho\": 450,\n",
    "    \"pizza\": 700,\n",
    "    \"pork_chop\": 450,\n",
    "    \"poutine\": 800,\n",
    "    \"prime_rib\": 650,\n",
    "    \"pulled_pork_sandwich\": 550,\n",
    "    \"ramen\": 550,\n",
    "    \"ravioli\": 500,\n",
    "    \"red_velvet_cake\": 430,\n",
    "    \"risotto\": 550,\n",
    "    \"samosa\": 260,\n",
    "    \"sashimi\": 200,\n",
    "    \"scallops\": 250,\n",
    "    \"seaweed_salad\": 120,\n",
    "    \"shrimp_and_grits\": 500,\n",
    "    \"spaghetti_bolognese\": 600,\n",
    "    \"spaghetti_carbonara\": 650,\n",
    "    \"spring_rolls\": 300,\n",
    "    \"steak\": 500,\n",
    "    \"strawberry_shortcake\": 350,\n",
    "    \"sushi\": 400,\n",
    "    \"tacos\": 450,\n",
    "    \"takoyaki\": 350,\n",
    "    \"tiramisu\": 450,\n",
    "    \"tuna_tartare\": 300,\n",
    "    \"waffles\": 500,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0458f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_calories(food_label: str) -> int:\n",
    "\n",
    "    if not isinstance(food_label, str):\n",
    "        return 400\n",
    "\n",
    "    food_label = food_label.lower().strip()\n",
    "\n",
    "    return FOOD_CALORIES.get(food_label, 400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "952d759a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown 0.5658053159713745 400\n"
     ]
    }
   ],
   "source": [
    "food, confidence = predict_food(\"../data/raw/p01/food-images/IMG_9404.jpeg\")\n",
    "calories = estimate_calories(food)\n",
    "\n",
    "print(food, confidence, calories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d4e09",
   "metadata": {},
   "source": [
    "Ce modèle est limité, car il n'est pas fine tuner sur les images des aliments !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a401bd",
   "metadata": {},
   "source": [
    "### ResNet152 food101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d2d3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, ResNetForImageClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_ID = \"fusion-bench/resnet152-food101-batch_size_64_lr_0.005_training_data_ratio_0.8-4000\" \n",
    "EXTRACT_DIR = \"../data/raw/p01/food-images/\"\n",
    "OUTPUT_CSV = \"../data/food/p01_resultats_classification_resnet152_food101.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "957de70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de l'appareil : cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de l'appareil : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3f18738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle ResNet-152 (fusion-bench/resnet152-food101-batch_size_64_lr_0.005_training_data_ratio_0.8-4000) chargé\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    processor = AutoImageProcessor.from_pretrained(MODEL_ID)\n",
    "    # Chargement du ResNet-152 avec les poids Food-101\n",
    "    model = ResNetForImageClassification.from_pretrained(MODEL_ID).to(device)\n",
    "    model.eval()\n",
    "    print(f\"Modèle ResNet-152 ({MODEL_ID}) chargé\")\n",
    "except Exception as e:\n",
    "    print(f\"ERREUR lors du chargement : {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f2518b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        return None, None, f\"Erreur de lecture: {e}\"\n",
    "\n",
    "    try:\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "        \n",
    "        predicted_class_idx = torch.argmax(probabilities).item()\n",
    "        \n",
    "        predicted_label = model.config.id2label[predicted_class_idx]\n",
    "        confidence_score = probabilities[predicted_class_idx].item() * 100\n",
    "\n",
    "        return predicted_label, confidence_score, \"OK\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None, None, f\"Erreur de classification: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03afd577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Début de la classification de 321 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classification des images: 100%|██████████| 321/321 [01:37<00:00,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "all_files = []\n",
    "for root, _, files in os.walk(EXTRACT_DIR):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff')):\n",
    "            all_files.append(os.path.join(root, file))\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"\\nDébut de la classification de {len(all_files)} images...\")\n",
    "\n",
    "for file_path in tqdm(all_files, desc=\"Classification des images\"):\n",
    "    # extraire la date/heure via EXIF\n",
    "    datetime_taken = extract_image_datetime(Path(file_path))\n",
    "    \n",
    "\n",
    "    # Extrait la date seule pour le nouveau champ \"Date_Prise\"\n",
    "    if isinstance(datetime_taken, pd.Timestamp):\n",
    "        date_taken = datetime_taken.date()\n",
    "    else:\n",
    "        date_taken = \"Non disponible\"\n",
    "\n",
    "    label, confidence, status = classify_image(file_path)\n",
    "\n",
    "    results.append({\n",
    "        \"Chemin_Fichier\": file_path,\n",
    "        \"Date_Prise\": date_taken,\n",
    "        \"Classe_Predite\": label,\n",
    "        \"Score_Confiance\": f\"{confidence:.2f}%\" if confidence is not None else None,\n",
    "        \"Statut\": status\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80590ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chemin_Fichier</th>\n",
       "      <th>Date_Prise</th>\n",
       "      <th>Classe_Predite</th>\n",
       "      <th>Score_Confiance</th>\n",
       "      <th>Statut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/raw/p01/food-images/IMG_8916.jpeg</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>macarons</td>\n",
       "      <td>12.93%</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/raw/p01/food-images/IMG_8917.jpeg</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>pancakes</td>\n",
       "      <td>22.30%</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/raw/p01/food-images/IMG_8918.jpeg</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>macarons</td>\n",
       "      <td>9.48%</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/raw/p01/food-images/IMG_8920.jpeg</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>pizza</td>\n",
       "      <td>21.25%</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/raw/p01/food-images/IMG_8921.jpeg</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>mussels</td>\n",
       "      <td>19.73%</td>\n",
       "      <td>OK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Chemin_Fichier  Date_Prise Classe_Predite  \\\n",
       "0  ../data/raw/p01/food-images/IMG_8916.jpeg  2020-02-01       macarons   \n",
       "1  ../data/raw/p01/food-images/IMG_8917.jpeg  2020-02-01       pancakes   \n",
       "2  ../data/raw/p01/food-images/IMG_8918.jpeg  2020-02-01       macarons   \n",
       "3  ../data/raw/p01/food-images/IMG_8920.jpeg  2020-02-01          pizza   \n",
       "4  ../data/raw/p01/food-images/IMG_8921.jpeg  2020-02-01        mussels   \n",
       "\n",
       "  Score_Confiance Statut  \n",
       "0          12.93%     OK  \n",
       "1          22.30%     OK  \n",
       "2           9.48%     OK  \n",
       "3          21.25%     OK  \n",
       "4          19.73%     OK  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_results['Chemin_Fichier'] = df_results['Chemin_Fichier'].str.replace(f'{EXTRACT_DIR}{os.sep}', '', regex=False)\n",
    "\n",
    "# Sauvegarde au format CSV\n",
    "df_results.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "display(df_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ac5e5",
   "metadata": {},
   "source": [
    "Le modèle ResNet-Food101 montre des performances de classification des aliments peu fiables. La précision est faible et meme les prédictions à haute confiance peuvent s'avérer incorrectes.\n",
    "\n",
    "La reconnaissance alimentaire est une tâche extrêmement complexe qui dépasse les capacités d'un modèle simple, notamment à cause de la grande diversité culinaire mondiale et de la difficulté technique d'estimer correctement les portions et par extension les calories.\n",
    "\n",
    "C'est précisément la complexité à laquelle s'attaque la startup Nutrify (Brisbane, Australie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d218b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f301b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08b0905a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
